---
title: "CompStat/R - Paper 1"
author: "Group 2: Carlo Michaelis, Patrick Molligo, Lukas Ruff"
date: "11 May 2016"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Part I

### 1. What are the atomic vector types in R? Explain which value they can take and give an example!

There are six *atomic* (or *basic*) vector types in R:

* **character**: Text, i.e. string variables.
* **numeric**: Real numbers, i.e. float variables.
* **integer**: Integers, i.e. values in  $\mathbb{Z}$.
* **complex**: Complex numbers, i.e. a pair of values with a real and imaginary part.
* **logical**: Boolean variables, i.e. either 1 (`TRUE`) or 0 (`FALSE`).
* **raw**: A raw vector contains fixed-length sequences of bytes.

**Examples**
```{r ExAtomicVectorTypes}
a <- c("blue", "red", "yellow")     ## character
b <- c(pi, exp(1), 0, 1)            ## numeric
c <- 1:10                           ## integer
d <- c(0+1i, 1+1i)                  ## complex
e <- c(TRUE, FALSE)                 ## logical
f <- raw(length = 3L)               ## raw
```

It is important to note, that a vector can only contain elements of the same type. We can check the type of an object using the `class`-function.

```{r CheckAtomicVectorTypes}
# verify types by using class function
lapply(list(a,b,c,d,e,f), class)
```

### 2. What is the difference between generic and atomic vectors?

  * An *atomic vector* can only contains objects of the same class. An example would be a vector which contains only integers.
  * A *generic vector* (in R representend as a `list`) can conatain objects of different classes. An example would be a vector which contains characters and numbers.

### 3. Explain the following statement: “A data frame is a list, but not every list is a data frame.”

  * A `list` is an object containing collections of objects. The types of the elements of the list can be different. It is for example allowed that a `list` contains a vector of real values (doubles) and a vector of characters. The length of the containing vectors can be *different*.
  * A `data frame` is also an object containing colletions of objects. The types of the elements of the list can also be different. But the length of the containing vectors have to be *the same*. We can think of a `data frame` as a table or matrix, where each row is an observation and each column a different variable. The length of each element or column are the number of rows or observations.

In conclusion `list` and `data frame` are very similar, but the `data frame` has one more restriction (same length of all vectors). That is why a `data frame` is always a `list`, but a `list` is not always a `data frame`.

## Part II

The following code will perform a simulation of 100'000'000 samples from a $\mathcal{N}(5,10)$ distribution, i.e. a normal distribution with mean $\mu = 5$ and standard deviation $\sigma = 10$. For reproducibility, we set a seed for the random number generator. In a second step, the cumulative sums of the first 100 samples are computed in two different ways, where the function `cumsum` returns a vector where element $i$ is the cumulative sum up to sample $i$. Finally, we check if the two ways of computing the cumulative sums up to sample 100 result in exactly equal vectors.

For random number generation R uses pseudo-random numbers. Starting from an initial state, called *seed state*, it will produce a deterministic sequence, which is used as random numbers. By choosing the same seed in every turn, we get the same results. To make the results of random numbers comparable, we first set the seed in a sepecific state, using `set.seed`.  
After setting the seed, we define a vector with (pseudo-) random values. Using the `rnorm` function we create the $1 \cdot 10^8$ normal distributed random values and save them in a vector called `largeVector`.

```{r NormSim1}
# Set the state of the random number generator (RNG) to 1
set.seed(1)

# Perform simulation of 1e8 samples from a normal distribution with mean 5
# and standard deviation 10
largeVector <- rnorm(1e6, mean=5, sd=10)
```

The function `cumsum`, which is used in the next code block, calculates the cumulative sum of the values of the vector. It takes all elements one by one and calculates for this element the sum of all elements before, including the current element. These values will be the new elements of the new vector. Consider following example:

$$
\left(\begin{array}{ccc}1\\ 4\\ 3\end{array}\right) \quad \overset{\texttt{cumsum}}{\xrightarrow{\hspace*{1cm}}} \quad \left(\begin{array}{ccc}1\\ 5\\ 8\end{array}\right)
$$

In case `a` it is doing `cumsum` of the whole vector `largeVector`. Afterwards it just takes the first 100 elements and saves them in vector `a`. In case `b` it first takes the 100 first elements of `largeVector` and calculates the `cumsum` afterwards, with only those 100 elements. The result is saved in vector `b`. In the end the two vectors `a` and `b` are checked for exact equality, using `identical` function.

```{r NormSim2}
# Compute the cumulative sums for the whole "largeVector" and subset the
# first 100 elements
a <- cumsum(largeVector)[1:100]

# Compute the cumulative sums only for the first 100 elements of
# "largeVector"
b <- cumsum(largeVector[1:100])

# Check, whether both ways of computation are exactly identical
identical(a, b)
```

Of course, both ways of computing the cumulative sums for the first $100$ samples above have the same result and hence `identical(a, b)` returns `TRUE`, but computation `a` is very inefficient compared to computation `b` since we first apply `cumsum` to the whole `largeVector`, i.e. we compute the cumulative sums for $100'000'000$ elements and then only look at the first $100$ elements. Computation `b` instead only computes the cumulative sums for the subset of the first $100$ elements directly.

In the following code, we stop the time for each of the two ways of computation using `system.time` function.

```{r Comp1}
# Computation method a
system.time(cumsum(largeVector)[1:100])
```

```{r Comp2}
# Computation method b
system.time(cumsum(largeVector[1:100]))
```

The *user* CPU time and the *system* CPU time is a technical distinction in time running the R code and time used in operating system kernel on behalf of the R code. The interesting time is the *elapsed* time, which is the sum of the *user* time and the *system* time. We can see that the first operation of taking the `cumsum` of the whole `largeVector` with its $100$ million elements (and reducing the vector to $100$ elements afterwards) takes a lot more CPU calucaltion time than taking the `cumsum` of the first $100$ elements directly.

The results prove our reasoning above, the second method is much more efficient than the first method, because finally end we are only interested in the `cumsum` of the first $100$ elements of the vector.

## Part III

We consider dataset from ``Munchner Mietspiegel 2003'' which contains $13$ variables about $2053$ flats in Munich. In the dataset the logical variables have following encoding: \lq yes\rq\ is $1$ and \lq no\rq\ is $0$. The variables are:

\begin{itemize}
\item \textbf{nm}: rent in EUR
\item \textbf{nmqm}: rent per $m^2$ in EUR
\item \textbf{wfl}: living space in $m^2$
\item \textbf{rooms}: number of rooms
\item \textbf{bj}: year of construction
\item \textbf{bez}: district
\item \textbf{wohngut}: good residential area (yes/no)
\item \textbf{wohnbest}: good residential area (yes/no)
\item \textbf{ww0}: water heating (yes/no)
\item \textbf{zh0}: central heating (yes/no)
\item \textbf{badkach0}: tiles in bathroom (yes/no)
\item \textbf{badextra}: optional extras in bathroom (yes/no)
\item \textbf{kueche}: luxury kitchen (yes/no)
\end{itemize}

\subsection{Data import and descriptive statistics}

First we read the data into our environment using \texttt{load} function. We will have a look to the raw data using \texttt{head} and we will get some first descriptive statistic information of the interval scaled variables using \texttt{summary} function.

<<>>=
load('miete.Rdata')
head(miete)
summary(miete$nm)
summary(miete$nmqm)
summary(miete$wfl)
summary(miete$rooms)
summary(miete$bj)
@

We get the \texttt{min}, the \texttt{max}, the first quantile, the third quantile, the \texttt{mean} and the \texttt{median}. If we also want to get some extra information like the standard deviation and maybe skew and kurtosis, we can use the \texttt{psych} library and the containing function \texttt{describe}. In this case we include all variables.

<<>>=
library(psych)
describe(miete)
@

With the above results we can also do a quick validation. The \texttt{min} and \texttt{max} of the logical (yes/no) variables should be $0$ and $1$ respectively, which is the case. To get the amount of missing values we can calulcate the \texttt{sum} of \texttt{is.na()}.

<<>>=
sum(is.na(miete))
@

There are no missing values in the whole dataset.

%% TODO: More validation? %%

\subsection{Identify relevant regressors and fit regression model}

To idenfity relevant regressors we can apply \texttt{lm()}, which calculates a linear model, to all variables. The first argument of the function is the formular. In our case we want to do a regression of the rent in EUR (\texttt{miete\$nm}) on all other variables (we can use the \texttt{.} to include all variables). In the second argument we set our dataset.

<<>>=
regrel <- lm(miete$nm ~ ., data = miete)
@

We can omit all variables which have no significant slope. To get the slope we can have a look to the \texttt{summary} of the result of the linear regression.

<<>>=
summary(regrel)
@

We would suggest to include all variables which are significant on a $99\%$ level (* or **). With the relevant variables we can fit the regression.

<<>>=
summary(lm(miete$nm ~ miete$nmqm + miete$wfl + miete$wohnbest +
             miete$ww0 + miete$kueche, data = miete))
@

\subsection{Discussion of model fit and interpretation}
