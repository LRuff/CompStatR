---
title: "CompStat/R - Paper 3"
author: "Group 2: Carlo Michaelis, Patrick Molligo, Lukas Ruff"
date: "06 July 2016"
fontsize: 11
lof: false
graphics: true
documentclass: article
output:
  pdf_document:
    latex_engine: pdflatex
    keep_tex: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE)
```


## Part I: Linear regression

In this first part of the paper, we will program a function which estimates the unknown parameters $\beta$ and $\sigma$ of a (ordinary) linear regression model

$$ y = X \beta + \varepsilon, \qquad \varepsilon \sim N(0, \sigma^2 I) $$

by the ordinary least squares (OLS) method.
For a given design matrix $X$ and response vector $y$ the OLS estimator is given by

\begin{equation} \label{eq:1}
\hat{\beta} = (X'X)^{-1}X'y
\end{equation}

with covariance matrix

\begin{equation} \label{eq:2}
\text{Var}(\hat{\beta}) = \sigma^2 (X'X)^{-1}
\end{equation}

where $\sigma^2$ has to be estimated via the sum of squared residuals ($SSR$):

\begin{equation} \label{eq:3}
\hat{\sigma}^2 = \frac{SSR}{df} = \frac{\sum_i (y_i - x_i' \hat{\beta})^2}{n-k}.
\end{equation}

The term $df$ refers to the degrees of freedom, i.e. the difference between the number of observations $n$ and the number of coefficients $k$.

### Raw implementation

The function `linModEst` is a raw implementation of the OLS estimator. The function takes the response vector $y$ (`y`) and design matrix $X$ (`x`) as arguments and returns a list with the following named elements:

* `coefficients`: the estimated coefficients $\hat{\beta}$
* `vcov`: the estimated covariance matrix $\widehat{\text{Var}(\hat{\beta)}}$
* `sigma`: the square root of the estimated scale parameter $\hat{\sigma}^2$
* `df`: the degrees of freedom $df$

We use equations (\ref{eq:1}), (\ref{eq:2}), and (\ref{eq:3}) for the implementation and compute the inverse of $X'X$ using the `solve` function, which numerically solves the equation
$$ (X'X) \, A = I $$
for the matrix $A = (X'X)^{-1}$. To efficiently compute $X'X$ and $X'y$, we use the `crossprod` function.

\newpage

```{r linModEst}
linModEst <- function(x, y) {
  # Computes the OLS estimator and sample variance assuming a (ordinary) linear
  # regression model.
  #
  # Args:
  #   x: design matrix x
  #   y: response vector y 
  #
  # Returns:
  #   A list with the following named elements:
  #     $coefficients: the estimated coefficients
  #     $vcov: the estimated covariance matrix
  #     $sigma: the square root of the estimated variance
  #     $df: the degrees of freedom in the model, i.e. the difference between 
  #          the number of rows and columns of x
  
  # Compute the inverse of (xâ€™x) using the solve- and crossprod-function
  inv <- solve(crossprod(x), diag(nrow = ncol(x)))
  
  # Compute beta hat, i.e. the estimated coefficients
  coefficients <- inv %*% crossprod(x, y)
  
  # Compute the degrees of freedom
  df <- nrow(x) - ncol(x)
  
  # Compute the sample variance via the sum of squared residuals (SSR)
  SSR <- sum((y - x %*% coefficients)^2)
  sigmaSquared <- SSR / df
  
  # Compute the covariance matrix
  vcov <- sigmaSquared * inv
  
  # Create named results list to be returned
  results <- list(coefficients, vcov, sqrt(sigmaSquared), df)
  names(results) <- c("coefficients", "vcov", "sigma", "df")
  
  # Return results
  results
}
```

We test our implementation by computing the linear relationship between heart weight, body weight and sex for the `cats` dataset contained in the package `MASS`. In the following piece of code, `cbind` combines its arguments by columns into a matrix with the number of columns given by the number of arguments and the number of rows given by the greatest length of the given arguments. Shorter arguments are repeated, as long as the matrix number of rows is a multiple of the shorter vector lengths.
Hence, `cbind(1, cats$Bwt, as.numeric(cats$Sex) - 1)` creates a design matrix with an intercept column, the variable body weight (`bwt`), and the variable sex (`Sex`), which is converted from a factor into a dummy variable using `as.numeric`. We subtract $1$ to receive dummy variable values of $0$ and $1$, rather than $1$ and $2$ from the original data. Thus, `cbind` is used to build a proper design matrix of object type `matrix` with an intercept and dummy variable, such that our implementation of `linModEst` works correctly.

\newpage

```{r linModEst Test1}
# Load cats dataset
data(cats, package = "MASS")

# Compute OLS using our implementation
linModEst(
  x = cbind(1, cats$Bwt, as.numeric(cats$Sex) - 1),
  y = cats$Hwt
)
```

We verify our results by comparing them to the output of `R`'s `lm` function:

```{r linModEst Test2}
summary(lm(Hwt ~ Bwt + Sex, data = cats))
```

As we can see, our implementation is correct.

### Extend implementation

In this section, we write a new function `linMod(formula, data)`, which estimates a linear regression model specified by `formula` and uses our `linModEst` function defined above to estimate the model parameters again by the OLS method. `linMod` returns a list with the following named elements:

* `coefficients`: named vector of the estimated coefficients $\hat{\beta}$
* `vcov`: named estimated covariance matrix $\widehat{\text{Var}(\hat{\beta)}}$
* `sigma`: the square root of the estimated scale parameter $\hat{\sigma}^2$
* `df`: the degrees of freedom $df$
* `formula`: the formula that represents the model equation
* `call`: the arguments with which `linMod` was called

Below, we use the `model.frame`, `model.extract`, and `model.matrix` functions, which are very convenient for working with objects of the class `formula`. `model.frame` returns a `data.frame` containing only the variables from its passed `data` argument, which are used in the `formula` expression given. The returned `data.frame` from the `model.frame` function has additional attributes, but these are not needed in our application. With `model.extract`, we are able to extract the response variable from the `data.frame` created by `model.frame`. Moreover, using `model.matrix`, we can create the design matrix (of object class `matrix`) again only from `formula` and `data` arguments. By default, the matrix returned by `model.matrix` includes an intercept and converts factor variables into proper dummy variables (i.e. a factor variable with $L$ levels results in $L-1$ dummy variables). More precisely, the default intercept is taken over from the `formula` object, which then by default adds an intercept term to the model equation, if not specified otherwise.
Finally, we use `match.call` to return the call of our function with all the specified arguments by their full names.

```{r linMod}
linMod <- function(formula, data) {
  # Computes the OLS estimator and sample variance assuming a (ordinary) linear
  # regression model with model equation specified by the formula-argument.
  #
  # Args:
  #   formula: a formula specifying the linear model equation
  #   data: a data.frame, list or environment, containing the variables used in
  #         formula
  #
  # Returns:
  #   A list with the following named elements:
  #     $coefficients: named vector of the estimated coefficients
  #     $vcov: named estimated covariance matrix
  #     $sigma: the square root of the estimated variance
  #     $df: the degrees of freedom in the model
  #     $formula: the formula that represents the model equation
  #     $call: the arguments with which the function was called
  
  # Extract the response variable using the model.extract function on the
  # data.frame returned by model.frame
  y <- model.extract(model.frame(formula, data = data), "response")
  
  # Create the design matrix using model.matrix, which overtakes an intercept
  # specified in the formula argument by default and converts factor variables into proper
  # dummy variables
  x <- model.matrix(formula, data = data)
  
  # Use previously defined linModEst for estimation
  tmp <- linModEst(x, y)
  
  # Prepare the output
  rownames(tmp$coefficients) <- colnames(x)
  colnames(tmp$vcov) <- colnames(x)
  rownames(tmp$vcov) <- colnames(x)
  
  # Create results list to be returned
  results <- c(tmp, formula, match.call())
  names(results) <- c("coefficients", "vcov", "sigma", "df", "formula", "call")
  
  # Return results
  results
}
```

Let's again test our implementation:

```{r linMod Test}
linMod(Hwt ~ Bwt + Sex, data = cats)
```

As we can see, the output has the desired format and the correct results.


\newpage
## Part II: S3 for linear models

In this section we will expand upon our linear regression function using one of `R`'s object oriented systems: S3. Our goal is to improve the function by returning a more concise output and ultimately replicating the results of the `lm` function. We'll start by redefining `linMod` from Part I and assigning it the class `linMod`.

```{r linMod Class}
# Begin with the same linMod function as in Part I
linMod <- function(formula, data) {
  
  y <- model.extract(model.frame(formula, data = data), "response")
  
  x <- model.matrix(formula, data = data)
  
  tmp <- linModEst(x, y)

  rownames(tmp$coefficients) <- colnames(x)
  colnames(tmp$vcov) <- colnames(x)
  rownames(tmp$vcov) <- colnames(x)
  
  results <- c(tmp, formula, match.call())
  names(results) <- c("coefficients", "vcov", "sigma", "df", "formula", "call")
  
# Use the class function to redefine the results with class "linMod"
  class(results) <- "linMod"

# Return results (of class "linMod")
  results
}
```

Now we'd like to define a printing method for all objects of class `linMod` so that the function returns a more readable and concise output.

```{r linMod Print Method}
# Define a printing method for linMod using the generic print function 
print.linMod <- function(x, ...) 
  
  # Use cat function to neatly return the function call and coefficients
  cat("Call:",
      "\n",
      deparse(x$call),
      "\n",
      "\nCoefficients:\n",
      rownames(x$coefficients), 
      "\n",
      round(x$coefficients, digits = 3), 
      sep = "\t")
```

Let's check the structure of `linMod`:

```{r linMod Class Test}
# Define the model to be estimated from Part I
modelFit <- linMod(Hwt ~ Bwt + Sex, data = cats)

# Verify the class of modelFit as well as the objects it contains 
str(modelFit)
```

Lastly, we simply want to print the model to see that the output is now clear and easy to read. We shouldn't have to explicitly use `print` either. `R` will recognize the newly defined `linMod` class and then locate the `linMod` *method* that we have written for the generic `print` function.

```{r print.linMod Test}
# Print modelFit
modelFit
```

The output is now much clearer and user-friendly. 


